{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVM9k86t4fT6"
   },
   "source": [
    "Nombre: Felix Gustavo Reyes\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "\r\n",
    "Implemtacion: Apache Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7F5CV-tdl2Uh"
   },
   "outputs": [],
   "source": [
    "!sudo apt install default-jre\r\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6WVJN4Nnl2Ro"
   },
   "outputs": [],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\r\n",
    "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\r\n",
    "!tar xf spark-2.4.7-bin-hadoop2.7.tgz\r\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJbz3jt5sCXR"
   },
   "outputs": [],
   "source": [
    "import os\r\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\r\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FOIk6mY_mC-C"
   },
   "outputs": [],
   "source": [
    "import findspark\r\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0vhpFs7EmXni"
   },
   "outputs": [],
   "source": [
    "import pyspark\r\n",
    "sc = pyspark.SparkContext(appName=\"yourAppName\")\r\n",
    "#reemplazarlo por el 7gigas\r\n",
    "lista = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fx8_tKesnEy8",
    "outputId": "d21c20ac-cada-4a97-dc2c-0b53b0a371e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creamos el rdd distribuido en RAM de los nodos\r\n",
    "rdd= sc.parallelize(lista) \r\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxCwHvz2nKa7"
   },
   "outputs": [],
   "source": [
    "#SparkSQL y DataFrames\r\n",
    "#MLib\r\n",
    "#GraphX\r\n",
    "#Spark Streming\r\n",
    "#Transformaciones .map() .filter()\r\n",
    "#Acciones: .count()  .first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6llN1YKTqNcU",
    "outputId": "19bd9e56-7a1c-46bc-d87d-bc2c27eff640"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5]"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filtrado aqui poner drill\r\n",
    "rdd_filtrado=rdd.filter(lambda x: x>1)\r\n",
    "rdd_filtrado.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4zyrzu4qNZ4"
   },
   "outputs": [],
   "source": [
    "#MAP\r\n",
    "def suma(x):\r\n",
    "  return(x+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wjyAfoi5qCs-",
    "outputId": "917d5f79-de41-475c-a38b-c279dd5054eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5, 6]"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_filtrado_sum=rdd_filtrado.map(suma)\r\n",
    "rdd_filtrado_sum.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aPxKMRX-qCqi",
    "outputId": "0e018889-7633-4892-ae04-a23c8d37d937"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 9), (4, 16), (5, 25), (6, 36)]"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_cuadrado=(rdd_filtrado.map(suma).map(lambda x:(x,x**2)) )\r\n",
    "rdd_cuadrado.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gamUYvhVqCle",
    "outputId": "e716a151-abb6-4d01-b42f-8c500f6bb614"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 9, 4, 16, 5, 25, 6, 36]"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Flat Map\r\n",
    "rdd_cuadrado=(rdd_filtrado.map(suma).flatMap(lambda x:(x,x**2)) )\r\n",
    "rdd_cuadrado.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ss4klkxqCij",
    "outputId": "7e84106e-0093-4c88-9b1e-99f86c2198c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 9, 4, 16, 5, 25, 6, 36]"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Muestras\r\n",
    "rdd_cuadrado.sample(False,1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_LZOYd0-xyyb",
    "outputId": "a5b784c8-3d30-4608-87fa-bcc33a583bb6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 9, 4, 5, 25, 6, 36]"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_cuadrado.sample(False,0.5).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CHbOunnMxyp0",
    "outputId": "09e94caf-fd4b-4acf-987e-9c36b31525b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 9, 4, 16, 5, 5, 5, 25, 6, 36]"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_cuadrado.sample(True,1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-1GEawQMxymx",
    "outputId": "c1f73d03-14f0-4146-d88a-2b1f13788c68"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 2, 3, 3, 4, 4]"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Quitar Duplicados\r\n",
    "lista=[1,1,2,2,3,3,4,4]\r\n",
    "rdd=sc.parallelize(lista)\r\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vTeALd26xet0",
    "outputId": "05e5fa9f-9437-483d-a1ae-5ceb4657f464"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 1, 3]"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0lwXRTo2xeq4",
    "outputId": "6dc4bef9-4225-4af6-a670-59ebec7e487b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(False, <pyspark.resultiterable.ResultIterable at 0x7f7ebf4447b8>),\n",
       " (True, <pyspark.resultiterable.ResultIterable at 0x7f7ebf444828>)]"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Agrupado de Datos en formato clave valor\r\n",
    "rdd_agrupado=rdd.groupBy(lambda x: x>1)\r\n",
    "rdd_agrupado.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UphRgAF0y_Km",
    "outputId": "c314e9e3-91b3-455f-ee73-472e0afd1ee2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(False, [1, 1]), (True, [2, 2, 3, 3, 4, 4])]\n"
     ]
    }
   ],
   "source": [
    "print([(x,sorted(y)) for (x,y) in rdd_agrupado.collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bNl_prr0y_IU",
    "outputId": "b9386258-e9aa-43f2-fa30-675c9111a8f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transformacion en varios RDD\r\n",
    "#Union\r\n",
    "rdda=sc.parallelize([1,2,3])\r\n",
    "rddb=sc.parallelize([4,5,6])\r\n",
    "rddu=rdda.union(rddb)\r\n",
    "rddu.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2nCQp7qoy_Fo",
    "outputId": "e66a5029-60ca-42da-c8e5-bc10f770f2d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Interseccion\r\n",
    "rdda=sc.parallelize([1,2,3])\r\n",
    "rddb=sc.parallelize([3,5,6])\r\n",
    "rddi=rdda.intersection(rddb)\r\n",
    "rddi.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KDcy06ZPz8L9",
    "outputId": "b938932e-87bf-448e-a120-6693ac0e8aa1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Substraccion\r\n",
    "rdds=rdda.subtract(rddb)\r\n",
    "rdds.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RJ_Zz5XVz8JO",
    "outputId": "32c2ce34-f7f4-4949-f655-26b8f9e1df41"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3), (1, 5), (1, 6), (2, 3), (3, 3), (2, 5), (2, 6), (3, 5), (3, 6)]"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Producto Cartesiano\r\n",
    "rddc=rdda.cartesian(rddb)\r\n",
    "rddc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9H_gSrHz8D-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHwVFIba56sm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDKPBmoY0znq"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"ParquetInputFormat\").getOrCreate()\r\n",
    "path = /user/root/ROLES\"\"\r\n",
    "#sc = spark.sparkContext()\r\n",
    "parquet_rdd = sc.newAPIHadoopFile(\r\n",
    "    path,\r\n",
    "    'org.apache.parquet.avro.AvroParquetInputFormat',\r\n",
    "    'java.lang.Void',\r\n",
    "    'org.apache.avro.generic.IndexedRecord',\r\n",
    "    valueConverter='org.apache.spark.examples.pythonconverters.IndexedRecordToJavaConverter')\r\n",
    "output = parquet_rdd.map(lambda x: x[1]).collect()\r\n",
    "for k in output:\r\n",
    "    print(k)\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p2tAMAh-MwG_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wWhIMWxVNLFh"
   },
   "outputs": [],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\r\n",
    "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\r\n",
    "!tar xf spark-2.4.7-bin-hadoop2.7.tgz\r\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kTu8IPzHMwCd"
   },
   "outputs": [],
   "source": [
    "!pip install -q findspark\r\n",
    "import os\r\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\r\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NnOqZda-LNUf"
   },
   "outputs": [],
   "source": [
    "import findspark\r\n",
    "findspark.init()\r\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eLkX--hVE5_S"
   },
   "outputs": [],
   "source": [
    "import pyspark\r\n",
    "from __future__ import print_function\r\n",
    "from pyspark.sql import SparkSession\r\n",
    "# $example on:schema_merging$\r\n",
    "from pyspark.sql import Row\r\n",
    "# $example off:schema_merging$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dK8ItJa1GLcw",
    "outputId": "924c502d-d756-409a-ffd6-fb5678b46f40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employees.json\tkv1.txt     people.json  user.avsc   users.orc\n",
      "full_user.avsc\tpeople.csv  people.txt\t users.avro  users.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\r\n",
    "path=\"/content/spark-2.4.7-bin-hadoop2.7/examples/src/main/resources/\"\r\n",
    "os.chdir(path)\r\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrViUKSMFH4x"
   },
   "source": [
    "###basic_datasource_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7wvgY8kR7bK4"
   },
   "outputs": [],
   "source": [
    "def basic_datasource_example(spark):\r\n",
    "    # $example on:generic_load_save_functions$\r\n",
    "    df = spark.read.load(\"/content/spark-2.4.7-bin-hadoop2.7/examples/src/main/resources/users.parquet\")\r\n",
    "    df.select(\"name\", \"favorite_color\").write.save(\"namesAndFavColors.parquet\")\r\n",
    "    # $example off:generic_load_save_functions$\r\n",
    "\r\n",
    "    # $example on:write_partitioning$\r\n",
    "    df.write.partitionBy(\"favorite_color\").format(\"parquet\").save(\"namesPartByColor.parquet\")\r\n",
    "    # $example off:write_partitioning$\r\n",
    "\r\n",
    "    # $example on:write_partition_and_bucket$\r\n",
    "    df = spark.read.parquet(\"/content/spark-2.4.7-bin-hadoop2.7/examples/src/main/resources/users.parquet\")\r\n",
    "    (df\r\n",
    "        .write\r\n",
    "        .partitionBy(\"favorite_color\")\r\n",
    "        .bucketBy(42, \"name\")\r\n",
    "        .saveAsTable(\"people_partitioned_bucketed\"))\r\n",
    "    # $example off:write_partition_and_bucket$\r\n",
    "\r\n",
    "    # $example on:manual_load_options$\r\n",
    "    df = spark.read.load(\"/content/spark-2.4.7-bin-hadoop2.7/examples/src/main/resources/people.json\", format=\"json\")\r\n",
    "    df.select(\"name\", \"age\").write.save(\"namesAndAges.parquet\", format=\"parquet\")\r\n",
    "    # $example off:manual_load_options$\r\n",
    "\r\n",
    "    # $example on:manual_load_options_csv$\r\n",
    "    df = spark.read.load(\"/content/spark-2.4.7-bin-hadoop2.7/examples/src/main/resources/people.csv\",\r\n",
    "                         format=\"csv\", sep=\":\", inferSchema=\"true\", header=\"true\")\r\n",
    "    # $example off:manual_load_options_csv$\r\n",
    "\r\n",
    "    # $example on:manual_save_options_orc$\r\n",
    "    df = spark.read.orc(\"/content/spark-2.4.7-bin-hadoop2.7/examples/src/main/resources/users.orc\")\r\n",
    "    (df.write.format(\"orc\")\r\n",
    "        .option(\"orc.bloom.filter.columns\", \"favorite_color\")\r\n",
    "        .option(\"orc.dictionary.key.threshold\", \"1.0\")\r\n",
    "        .save(\"users_with_options.orc\"))\r\n",
    "    # $example off:manual_save_options_orc$\r\n",
    "\r\n",
    "    # $example on:write_sorting_and_bucketing$ \r\n",
    "    #name, favorite_color, favorite_numbers\r\n",
    "    #df.write.bucketBy(42, \"name\").sortBy(\"age\").saveAsTable(\"people_bucketed\")\r\n",
    "    df.write.bucketBy(42, \"name\").sortBy(\"favorite_color\").saveAsTable(\"people_bucketed\")    \r\n",
    "    # $example off:write_sorting_and_bucketing$\r\n",
    "\r\n",
    "    # $example on:direct_sql$\r\n",
    "    df = spark.sql(\"SELECT * FROM parquet.`/content/spark-2.4.7-bin-hadoop2.7/examples/src/main/resources/users.parquet`\")\r\n",
    "    # $example off:direct_sql$\r\n",
    "\r\n",
    "    spark.sql(\"DROP TABLE IF EXISTS people_bucketed\")\r\n",
    "    spark.sql(\"DROP TABLE IF EXISTS people_partitioned_bucketed\")\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPMGetPXFRZn"
   },
   "source": [
    "###parquet_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZDkRzKcFS8u"
   },
   "outputs": [],
   "source": [
    "def parquet_example(spark):\r\n",
    "    # $example on:basic_parquet_example$\r\n",
    "    peopleDF = spark.read.json(\"/content/spark-2.4.7-bin-hadoop2.7/examples/src/main/resources/people.json\")\r\n",
    "\r\n",
    "    # DataFrames can be saved as Parquet files, maintaining the schema information.\r\n",
    "    peopleDF.write.parquet(\"people.parquet\")\r\n",
    "\r\n",
    "    # Read in the Parquet file created above.\r\n",
    "    # Parquet files are self-describing so the schema is preserved.\r\n",
    "    # The result of loading a parquet file is also a DataFrame.\r\n",
    "    parquetFile = spark.read.parquet(\"/content/spark-2.4.7-bin-hadoop2.7/examples/src/main/resources/people.parquet\")\r\n",
    "\r\n",
    "    # Parquet files can also be used to create a temporary view and then used in SQL statements.\r\n",
    "    parquetFile.createOrReplaceTempView(\"parquetFile\")\r\n",
    "    teenagers = spark.sql(\"SELECT name FROM parquetFile WHERE age >= 13 AND age <= 19\")\r\n",
    "    teenagers.show()\r\n",
    "    # +------+\r\n",
    "    # |  name|\r\n",
    "    # +------+\r\n",
    "    # |Justin|\r\n",
    "    # +------+\r\n",
    "    # $example off:basic_parquet_example$\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YppiuJgGFTxp"
   },
   "source": [
    "###parquet_schema_merging_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywoqJNr-Fa1c"
   },
   "outputs": [],
   "source": [
    "def parquet_schema_merging_example(spark):\r\n",
    "    # $example on:schema_merging$\r\n",
    "    # spark is from the previous example.\r\n",
    "    # Create a simple DataFrame, stored into a partition directory\r\n",
    "    sc = spark.sparkContext\r\n",
    "\r\n",
    "    squaresDF = spark.createDataFrame(sc.parallelize(range(1, 6))\r\n",
    "                                      .map(lambda i: Row(single=i, double=i ** 2)))\r\n",
    "    squaresDF.write.parquet(\"data/test_table/key=1\")\r\n",
    "\r\n",
    "    # Create another DataFrame in a new partition directory,\r\n",
    "    # adding a new column and dropping an existing column\r\n",
    "    cubesDF = spark.createDataFrame(sc.parallelize(range(6, 11))\r\n",
    "                                    .map(lambda i: Row(single=i, triple=i ** 3)))\r\n",
    "    cubesDF.write.parquet(\"data/test_table/key=2\")\r\n",
    "\r\n",
    "    # Read the partitioned table\r\n",
    "    mergedDF = spark.read.option(\"mergeSchema\", \"true\").parquet(\"data/test_table\")\r\n",
    "    mergedDF.printSchema()\r\n",
    "\r\n",
    "    # The final schema consists of all 3 columns in the Parquet files together\r\n",
    "    # with the partitioning column appeared in the partition directory paths.\r\n",
    "    # root\r\n",
    "    #  |-- double: long (nullable = true)\r\n",
    "    #  |-- single: long (nullable = true)\r\n",
    "    #  |-- triple: long (nullable = true)\r\n",
    "    #  |-- key: integer (nullable = true)\r\n",
    "    # $example off:schema_merging$\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsnOBZPlF0Wn"
   },
   "source": [
    "###json_dataset_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSb1AyC3FqbX"
   },
   "outputs": [],
   "source": [
    "def json_dataset_example(spark):\r\n",
    "    # $example on:json_dataset$\r\n",
    "    # spark is from the previous example.\r\n",
    "    sc = spark.sparkContext\r\n",
    "\r\n",
    "    # A JSON dataset is pointed to by path.\r\n",
    "    # The path can be either a single text file or a directory storing text files\r\n",
    "    path = \"/content/spark-2.4.7-bin-hadoop2.7/examples/src/main/resources/people.json\"\r\n",
    "    peopleDF = spark.read.json(path)\r\n",
    "\r\n",
    "    # The inferred schema can be visualized using the printSchema() method\r\n",
    "    peopleDF.printSchema()\r\n",
    "    # root\r\n",
    "    #  |-- age: long (nullable = true)\r\n",
    "    #  |-- name: string (nullable = true)\r\n",
    "\r\n",
    "    # Creates a temporary view using the DataFrame\r\n",
    "    peopleDF.createOrReplaceTempView(\"people\")\r\n",
    "\r\n",
    "    # SQL statements can be run by using the sql methods provided by spark\r\n",
    "    teenagerNamesDF = spark.sql(\"SELECT name FROM people WHERE age BETWEEN 13 AND 19\")\r\n",
    "    teenagerNamesDF.show()\r\n",
    "    # +------+\r\n",
    "    # |  name|\r\n",
    "    # +------+\r\n",
    "    # |Justin|\r\n",
    "    # +------+\r\n",
    "\r\n",
    "    # Alternatively, a DataFrame can be created for a JSON dataset represented by\r\n",
    "    # an RDD[String] storing one JSON object per string\r\n",
    "    jsonStrings = ['{\"name\":\"Yin\",\"address\":{\"city\":\"Columbus\",\"state\":\"Ohio\"}}']\r\n",
    "    otherPeopleRDD = sc.parallelize(jsonStrings)\r\n",
    "    otherPeople = spark.read.json(otherPeopleRDD)\r\n",
    "    otherPeople.show()\r\n",
    "    # +---------------+----+\r\n",
    "    # |        address|name|\r\n",
    "    # +---------------+----+\r\n",
    "    # |[Columbus,Ohio]| Yin|\r\n",
    "    # +---------------+----+\r\n",
    "    # $example off:json_dataset$\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mY7Dv5TF3ED"
   },
   "source": [
    "###jdbc_dataset_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Afkg7HG0X_qn",
    "outputId": "a3f9663d-4661-45ee-ceb0-1b105dc0f6f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-12-13 04:35:18--  https://jdbc.postgresql.org/download/postgresql-42.2.6.jar\n",
      "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
      "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 842825 (823K) [application/java-archive]\n",
      "Saving to: ‘postgresql-42.2.6.jar’\n",
      "\n",
      "postgresql-42.2.6.j 100%[===================>] 823.07K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2020-12-13 04:35:18 (7.86 MB/s) - ‘postgresql-42.2.6.jar’ saved [842825/842825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://jdbc.postgresql.org/download/postgresql-42.2.6.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GUU0ZH_BnLmB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c5I2_aT5FqX_"
   },
   "outputs": [],
   "source": [
    "def jdbc_dataset_example(spark):\r\n",
    "    # $example on:jdbc_dataset$\r\n",
    "    # Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods\r\n",
    "    # Loading data from a JDBC source    \r\n",
    "    #.option(\"query\", \"SELECT *  FROM tblDatos\")\r\n",
    "    jdbcDF = spark.read \\\r\n",
    "        .format(\"jdbc\") \\\r\n",
    "        .option(\"database\", \"\") \\\r\n",
    "        .option(\"dbtable\", \"\") \\\r\n",
    "        .option(\"url\", \"\") \\\r\n",
    "        .option(\"user\", \"\") \\\r\n",
    "        .option(\"password\", \"\") \\\r\n",
    "        .load()\r\n",
    "\r\n",
    "        #.option(\"encrypt\", \"true\") \\\r\n",
    "        #.option(\"trustServerCertificate\", \"false\") \\\r\n",
    "        #.option(\"hostNameInCertificate\", \"*.database.windows.net\") \\\r\n",
    "        #.option(\"loginTimeout\", \"30\") \\\r\n",
    "\r\n",
    "    #jdbcDF2 = spark.read \\\r\n",
    "    #    .jdbc(\"jdbc:postgresql:dbserver\", \"schema.tablename\",\r\n",
    "    #          properties={\"user\": \"username\", \"password\": \"password\"})\r\n",
    "\r\n",
    "    # Specifying dataframe column data types on read\r\n",
    "    #jdbcDF3 = spark.read \\\r\n",
    "    #    .format(\"jdbc\") \\\r\n",
    "    #    .option(\"url\", \"jdbc:postgresql:dbserver\") \\\r\n",
    "    #    .option(\"dbtable\", \"schema.tablename\") \\\r\n",
    "    #    .option(\"user\", \"username\") \\\r\n",
    "    #    .option(\"password\", \"password\") \\\r\n",
    "    #    .option(\"customSchema\", \"id DECIMAL(38, 0), name STRING\") \\\r\n",
    "    #    .load()\r\n",
    "\r\n",
    "    # Saving data to a JDBC source\r\n",
    "    #jdbcDF.write \\\r\n",
    "    #    .format(\"jdbc\") \\\r\n",
    "    #    .option(\"url\", \"jdbc:postgresql:dbserver\") \\\r\n",
    "    #    .option(\"dbtable\", \"schema.tablename\") \\\r\n",
    "    #    .option(\"user\", \"username\") \\\r\n",
    "    #    .option(\"password\", \"password\") \\\r\n",
    "    #    .save()\r\n",
    "\r\n",
    "    #jdbcDF2.write \\\r\n",
    "    #    .jdbc(\"jdbc:postgresql:dbserver\", \"schema.tablename\",\r\n",
    "    #          properties={\"user\": \"username\", \"password\": \"password\"})\r\n",
    "\r\n",
    "    # Specifying create table column data types on write\r\n",
    "    #jdbcDF.write \\\r\n",
    "    #    .option(\"createTableColumnTypes\", \"name CHAR(64), comments VARCHAR(1024)\") \\\r\n",
    "    #    .jdbc(\"jdbc:postgresql:dbserver\", \"schema.tablename\",\r\n",
    "    #          properties={\"user\": \"username\", \"password\": \"password\"})\r\n",
    "    # $example off:jdbc_dataset$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFBi7gFXF652"
   },
   "source": [
    "###Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZrnE2VjEmkG"
   },
   "outputs": [],
   "source": [
    "    spark = SparkSession.builder.appName(\"Python Spark SQL data source example\").getOrCreate()\r\n",
    "    basic_datasource_example(spark)\r\n",
    "    parquet_example(spark)\r\n",
    "    parquet_schema_merging_example(spark)\r\n",
    "    json_dataset_example(spark)\r\n",
    "    #jdbc_dataset_example(spark)\r\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHLTHgaDEl0j"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HHG5BaHlElxb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_-UPgku5Eluh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "SrViUKSMFH4x",
    "NPMGetPXFRZn",
    "YppiuJgGFTxp",
    "AsnOBZPlF0Wn"
   ],
   "name": "AdminProjects2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
